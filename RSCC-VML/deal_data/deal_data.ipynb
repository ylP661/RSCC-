{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import json\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_prompt(mask_path,caption):\n",
    "    \n",
    "    caption=caption.replace('.','').strip()\n",
    "    prompt_add=\"Please refer to this description:{}.Modify or correct it.\".format(caption)\n",
    "\n",
    "    return prompt_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10077/10077 [00:20<00:00, 480.55it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_single_data(prompt,img1,img2,answer):\n",
    "\n",
    "    block={\n",
    "            \"conversations\":[\n",
    "                {\n",
    "                    \"from\": \"human\",\n",
    "                    \"value\": prompt\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"gpt\",\n",
    "                    \"value\": answer\n",
    "                }\n",
    "            ],\n",
    "            \"images\": [\n",
    "                img1,\n",
    "                img2\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    return block\n",
    "\n",
    "def get_multi_data(prompt,img1,img2,answer):\n",
    "\n",
    "    block={\n",
    "            \"conversations\":[\n",
    "                {\n",
    "                    \"from\": \"human\",\n",
    "                    \"value\": prompt['Is']\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"gpt\",\n",
    "                    \"value\": answer['Is']\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"human\",\n",
    "                    \"value\": prompt['road']\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"gpt\",\n",
    "                    \"value\": answer['road']\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"human\",\n",
    "                    \"value\": prompt['build']\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"gpt\",\n",
    "                    \"value\": answer['build']\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"human\",\n",
    "                    \"value\": prompt['caption']\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"gpt\",\n",
    "                    \"value\": answer['caption']\n",
    "                }\n",
    "            ],\n",
    "            \"images\": [\n",
    "                img1,\n",
    "                img2\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    return block\n",
    "\n",
    "def getChageMask(mask_path):\n",
    "    changeMask = Image.open(mask_path)\n",
    "    changeMask = np.array(changeMask)\n",
    "    return changeMask,changeMask.shape\n",
    "\n",
    "def analyze_segmentation_mask(mask,flag=True):\n",
    "    # 目标颜色定义\n",
    "    if flag:\n",
    "        ROAD = [128, 128, 128]\n",
    "        BUILDING = [255, 255, 255]\n",
    "        BACKGROUND = [0, 0, 0]\n",
    "    else:\n",
    "        ROAD = [255, 255,0]\n",
    "        BUILDING = [255, 0, 0]\n",
    "        BACKGROUND = [0, 0, 0]\n",
    "\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "import pickle \n",
    "import json\n",
    "\n",
    "extra_caption=pickle.load(open('/data/coding/datasets/extra/extra_info/extra_caption.pickle','rb'))\n",
    "data=json.load(open('datasets/LEVIR-CC-dataset/LevirCCcaptions.json','r'))\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "outlist=[]\n",
    "\n",
    "answer_dict={}\n",
    "\n",
    "for i in tqdm(data['images']):\n",
    "\n",
    "    filepath=i['filepath']\n",
    "\n",
    "    filename=i['filename']\n",
    "    answer=i['sentences'][0]['raw']\n",
    "\n",
    "    truth_list=[]\n",
    "    truth_list_all=[]\n",
    "    \n",
    "\n",
    "    for j in i['sentences']:\n",
    "        truth_list.append(j['raw'].replace('.','').strip().split())\n",
    "\n",
    "    out_compute=extra_caption[filename]\n",
    "        \n",
    "\n",
    "    smooth_fn = SmoothingFunction().method1 \n",
    "\n",
    "    bleu_list=[]\n",
    "\n",
    "    for j in truth_list:\n",
    "        bleu_list.append(sentence_bleu([j], out_compute.split(), smoothing_function=smooth_fn,weights=(1,0,0,0))+sentence_bleu([j], out_compute.split(), smoothing_function=smooth_fn,weights=(0,1,0,0))+sentence_bleu([j], out_compute.split(), smoothing_function=smooth_fn,weights=(0,0,1,0))+sentence_bleu([j], out_compute.split(), smoothing_function=smooth_fn,weights=(0,0,0,1)))\n",
    "\n",
    "    argmax=np.argmax(np.array(bleu_list))\n",
    "\n",
    "    answer_dict[filename]=i['sentences'][argmax]['raw'].replace('.','').strip()\n",
    "\n",
    "import pickle\n",
    "\n",
    "data=json.load(open('datasets/LEVIR-CC-dataset/LevirCCcaptions.json','r'))\n",
    "\n",
    "for i in tqdm(data['images']):\n",
    "\n",
    "    filepath=i['filepath']\n",
    "    filename=i['filename']\n",
    "\n",
    "    mask_path='/data/coding/datasets/LEVIR-MCI-dataset/images/'+filepath+'/label/'+filename\n",
    "    mask,img_shape=getChageMask(mask_path)\n",
    "    has_target, target_counts, contours, road_contours_simplified, building_contours_simplified=analyze_segmentation_mask(mask)\n",
    "\n",
    "    road_loc=determine_position(img_shape,road_contours_simplified)\n",
    "    building_loc=determine_position(img_shape,building_contours_simplified)\n",
    "\n",
    "    a_data=(has_target, target_counts, contours, road_contours_simplified, building_contours_simplified,img_shape,road_loc,building_loc)\n",
    "\n",
    "    with open('/data/coding/datasets/LEVIR-CC-dataset/extra_info/'+filename,'wb') as f:\n",
    "        pickle.dump(a_data,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10077/10077 [00:20<00:00, 482.09it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "outlist=[]\n",
    "\n",
    "answer_dict={}\n",
    "\n",
    "for i in tqdm(data['images']):\n",
    "\n",
    "    filepath=i['filepath']\n",
    "\n",
    "\n",
    "\n",
    "    filename=i['filename']\n",
    "    answer=i['sentences'][0]['raw']\n",
    "\n",
    "    truth_list=[]\n",
    "    truth_list_all=[]\n",
    "    \n",
    "\n",
    "    for j in i['sentences']:\n",
    "        truth_list.append(j['raw'].replace('.','').strip().split())\n",
    "\n",
    "    out_compute=extra_caption[filename]\n",
    "        \n",
    "\n",
    "    smooth_fn = SmoothingFunction().method1  # 使用平滑方法\n",
    "\n",
    "    bleu_list=[]\n",
    "\n",
    "    for j in truth_list:\n",
    "        bleu_list.append(sentence_bleu([j], out_compute.split(), smoothing_function=smooth_fn,weights=(1,0,0,0))+sentence_bleu([j], out_compute.split(), smoothing_function=smooth_fn,weights=(0,1,0,0))+sentence_bleu([j], out_compute.split(), smoothing_function=smooth_fn,weights=(0,0,1,0))+sentence_bleu([j], out_compute.split(), smoothing_function=smooth_fn,weights=(0,0,0,1)))\n",
    "\n",
    "    #print(bleu_list)\n",
    "    argmax=np.argmax(np.array(bleu_list))\n",
    "\n",
    "    answer_dict[filename]=i['sentences'][argmax]['raw'].replace('.','').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10077 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10077/10077 [00:01<00:00, 9248.15it/s]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "funtine_data_train_caption=[]           ### change caption\n",
    "funtine_data_train_other_loc_road=[] \n",
    "funtine_data_train_other_loc_build=[] \n",
    "funtine_data_train_mutil=[]             ### 多轮对话\n",
    "\n",
    "for i in tqdm(data['images']):\n",
    "\n",
    "    ####\n",
    "    multi_prompt={}\n",
    "    multi_answer={}\n",
    "    ####\n",
    "\n",
    "    filepath=i['filepath']\n",
    "    filename=i['filename']\n",
    "\n",
    "    if filepath=='test':\n",
    "        continue\n",
    "\n",
    "    caption=extra_caption[filename]\n",
    "    ppp=convert_prompt('/data/coding/datasets/extra/'+filename,caption)\n",
    "\n",
    "    img1='/data/coding/datasets/LEVIR-CC-dataset/images/'+filepath+'/A/'+filename\n",
    "    img2='/data/coding/datasets/LEVIR-CC-dataset/images/'+filepath+'/B/'+filename\n",
    "\n",
    "    ### change caption\n",
    "\n",
    "    caption_list=[]\n",
    "\n",
    "    if i['changeflag']==0:\n",
    "        answer='the scene is the same as before'\n",
    "\n",
    "        block=get_single_data(ppp+\"Please describe the changes in <image> and <image>\",img1,img2,answer)\n",
    "\n",
    "        funtine_data_train_caption.append(block)\n",
    "\n",
    "    else:\n",
    "        \n",
    "        answer=answer_dict[filename]\n",
    "\n",
    "        block=get_single_data(ppp+\"Please describe the changes in <image> and <image>\",img1,img2,answer) \n",
    "        \n",
    "        funtine_data_train_caption.append(block)\n",
    "\n",
    "      \n",
    "    ######判断是否有改变\n",
    "\n",
    "    if i['changeflag']==1:\n",
    "        answer='Yes'\n",
    "    else:\n",
    "        answer='No'\n",
    "\n",
    "    block=get_single_data(\"Please check if there have been any changes to the roads or buildings in these two images:<image> and <image>. Please answer yes or no.\",img1,img2,answer)\n",
    "\n",
    "    funtine_data_train_class.append(block)\n",
    "    \n",
    "    #if i['changeflag']==1:\n",
    "    multi_prompt['Is']=\"Have there been any changes to the roads and buildings in these two images:<image> and <image>?\"\n",
    "    multi_answer['Is']=answer\n",
    "\n",
    "\n",
    "    \n",
    "    has_target, target_counts, contours, road_contours_simplified, building_contours_simplified,img_shape,road_loc,building_loc=pickle.load(open('/data/coding/datasets/LEVIR-MCI-dataset/extra_info/'+filename,'rb'))\n",
    "\n",
    "    if target_counts['road']!=0:\n",
    "        load_answer=\"There are a total of {} new road here.\".format(target_counts['road'])\n",
    "    else:\n",
    "        load_answer='There are no changes in roads,'\n",
    "\n",
    "    if target_counts['building']!=0:\n",
    "        building_answer=\"There are a total of {} new buildings here.\".format(target_counts['building'])\n",
    "    else:\n",
    "        building_answer='There are no changes in building.'\n",
    "\n",
    "    answer=i['sentences'][0]['raw'].replace('.','').strip()\n",
    "    if i['changeflag']==0:\n",
    "        answer='There are no changes in roads or buildings between the two images'\n",
    "    else:\n",
    "        answer=load_answer+building_answer\n",
    "\n",
    "    block=get_single_data(\"Please determine how many roads and buildings have changed between the two images:<image> and <image>\",img1,img2,answer)\n",
    "\n",
    "    funtine_data_train_count.append(block)\n",
    "\n",
    "\n",
    "    if len(road_loc)!=0:\n",
    "        counter = Counter(road_loc)\n",
    "        load_answer=\"There are a total of {} new road here.There is\".format(len(road_loc))\n",
    "        for j in list(counter.keys()):\n",
    "            load_answer+=\" {} new road located in the {},\".format(counter[j],j)\n",
    "    else:\n",
    "        load_answer='There are no changes in roads,'\n",
    "\n",
    "    if len(building_loc)!=0:\n",
    "        counter = Counter(building_loc)\n",
    "        #print(counter)\n",
    "        building_answer=\"There are a total of {} new buildings here.There is\".format(len(building_loc))\n",
    "        for j in list(counter.keys()):\n",
    "            building_answer+=\" {} new building located in the {},\".format(counter[j],j)\n",
    "        building_answer=building_answer[:-1]+'.' \n",
    "    else:\n",
    "        building_answer='There are no changes in building.'\n",
    "\n",
    "    answer=i['sentences'][0]['raw'].replace('.','').strip()\n",
    "\n",
    "    if i['changeflag']==0:\n",
    "        answer='There are no changes in roads or buildings between the two images'\n",
    "    else:\n",
    "        answer=load_answer+building_answer\n",
    "\n",
    "    block=get_single_data(\"Please identify the location of roads and buildings in the changes in these two images:<image> and <image>\",img1,img2,answer)\n",
    "\n",
    "    funtine_data_train_loc.append(block)\n",
    "\n",
    "    #break\n",
    "\n",
    "    #####  road\n",
    "    if len(road_loc)!=0:\n",
    "        counter = Counter(road_loc)\n",
    "\n",
    "        block_list=[]\n",
    "\n",
    "        funtine_data_train_other_count_road.append(get_single_data(\"How many roads have been changed in these two images:<image> and <image>\",img1,img2,str(len(road_loc))))\n",
    "\n",
    "        block_list.append([\"How many roads have been changed in these two images?\",str(len(road_loc))])\n",
    "\n",
    "        answer=''\n",
    "        for j in list(counter.keys()):\n",
    "            answer+=j+','\n",
    "\n",
    "        answer=answer[:-1]\n",
    "        \n",
    "        funtine_data_train_other_loc_road.append(get_single_data(\"Please tell me where the roads have changed in these two images:<image> and <image>\",img1,img2,answer))\n",
    "\n",
    "        block_list.append([\"Please tell me where the roads have changed in these two images\",answer])\n",
    "\n",
    "        bbb=random.choice(block_list)\n",
    "\n",
    "        multi_prompt['road']=bbb[0]\n",
    "        multi_answer['road']=bbb[1]\n",
    "\n",
    "        \n",
    "    else:\n",
    "        answer='0'\n",
    "\n",
    "        block_list=[]\n",
    "\n",
    "        funtine_data_train_other_count_road.append(get_single_data(\"How many roads have been changed in these two images:<image> and <image>\",img1,img2,answer))\n",
    "\n",
    "        block_list.append([\"How many roads have been changed in these two images?\",answer])\n",
    "\n",
    "        funtine_data_train_other_loc_road.append(get_single_data(\"Please tell me where the roads have changed in these two images:<image> and <image>,if no change please output: No change.\",img1,img2,\"No change\"))\n",
    "\n",
    "        block_list.append([\"Please tell me where the roads have changed in these two images\",\"No change\"])\n",
    "    \n",
    "        bbb=random.choice(block_list)\n",
    "\n",
    "        multi_prompt['road']=bbb[0]\n",
    "        multi_answer['road']=bbb[1]\n",
    "    \n",
    "        ###\n",
    "\n",
    "    #####  building\n",
    "    if len(building_loc)!=0:\n",
    "\n",
    "        block_list=[]\n",
    "\n",
    "        counter = Counter(building_loc)\n",
    "        funtine_data_train_other_count_build.append(get_single_data(\"How many buildings have been changed in these two images:<image> and <image>\",img1,img2,str(len(building_loc))))\n",
    "        \n",
    "        block_list.append([\"How many buildings have been changed in these two images?\",str(len(building_loc))])\n",
    "        \n",
    "        answer=''\n",
    "        for j in list(counter.keys()):\n",
    "            answer+=j+','\n",
    "\n",
    "        answer=answer[:-1]\n",
    "        \n",
    "        funtine_data_train_other_loc_build.append(get_single_data(\"Please tell me where the buildings have changed in these two images:<image> and <image>\",img1,img2,answer))\n",
    "\n",
    "        block_list.append([\"Please tell me where the buildings have changed in these two images\",answer])\n",
    "\n",
    "        bbb=random.choice(block_list)\n",
    "\n",
    "        multi_prompt['build']=bbb[0]\n",
    "        multi_answer['build']=bbb[1]\n",
    "\n",
    "    else:\n",
    "        answer='0'\n",
    "        block_list=[]\n",
    "        funtine_data_train_other_count_build.append(get_single_data(\"How many buildings have been changed in these two images:<image> and <image>\",img1,img2,answer))\n",
    "        block_list.append([\"How many roads have been changed in these two images?\",answer])\n",
    "        funtine_data_train_other_loc_build.append(get_single_data(\"Please tell me where the buildings have changed in these two images:<image> and <image>,if no change please output: No change.\",img1,img2,\"No change\"))\n",
    "\n",
    "        block_list.append([\"Please tell me where the roads have changed in these two images\",\"No change\"])\n",
    "    \n",
    "        bbb=random.choice(block_list)\n",
    "\n",
    "        multi_prompt['build']=bbb[0]\n",
    "        multi_answer['build']=bbb[1]\n",
    "\n",
    "        \n",
    "  \n",
    "    \n",
    "        \n",
    "    multi_prompt['caption']=ppp+'Please describe the changes.'\n",
    "    multi_answer['caption']=answer_dict[filename]\n",
    "\n",
    "    funtine_data_train_mutil.append(get_multi_data(multi_prompt,img1,img2,multi_answer))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('muti_task_data/train_task_data/caption.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(funtine_data_train_caption, json_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10077/10077 [00:02<00:00, 4431.44it/s]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "funtine_data_train_caption=[]           ### change caption\n",
    "funtine_data_train_other_loc_road=[] \n",
    "funtine_data_train_other_loc_build=[] \n",
    "funtine_data_train_mutil=[]             ### 多轮对话\n",
    "\n",
    "for i in tqdm(data['images']):\n",
    "\n",
    "    ####\n",
    "    multi_prompt={}\n",
    "    multi_answer={}\n",
    "    ####\n",
    "\n",
    "    filepath=i['filepath']\n",
    "    filename=i['filename']\n",
    "\n",
    "    if filepath=='test':\n",
    "        continue\n",
    "\n",
    "    caption=extra_caption[filename]\n",
    "    ppp=convert_prompt('/data/coding/datasets/extra/'+filename,caption)\n",
    "\n",
    "    img1='/data/coding/datasets/LEVIR-CC-dataset/images/'+filepath+'/A/'+filename\n",
    "    img2='/data/coding/datasets/LEVIR-CC-dataset/images/'+filepath+'/B/'+filename\n",
    "\n",
    "    ### change caption\n",
    "\n",
    "    caption_list=[]\n",
    "\n",
    "    if i['changeflag']==0:\n",
    "        answer='the scene is the same as before'\n",
    "\n",
    "        block=get_single_data(\"Please describe the changes in <image> and <image>\",img1,img2,answer)\n",
    "\n",
    "        funtine_data_train_caption.append(block)\n",
    "\n",
    "        caption_list.append(answer)\n",
    "\n",
    "    else:\n",
    "        \n",
    "        answer=''\n",
    "        \n",
    "        for idx,j in enumerate(i['sentences']):\n",
    "            answer=j['raw'].replace('.','').strip()\n",
    "\n",
    "            block=get_single_data(\"Please describe the changes in <image> and <image>\",img1,img2,answer) \n",
    "        \n",
    "            funtine_data_train_caption.append(block)\n",
    "\n",
    "            caption_list.append(answer)\n",
    "    if len(caption_list) != 0:\n",
    "        for kkk in caption_list:\n",
    "            multi_prompt['caption']='Please describe the changes.'\n",
    "            multi_answer['caption']=kkk\n",
    "\n",
    "            funtine_data_train_mutil.append(get_multi_data(multi_prompt,img1,img2,multi_answer))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('muti_task_data/test_task_data/loc_road.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(funtine_data_train_other_loc_road, json_file, ensure_ascii=False, indent=4)\n",
    "with open('muti_task_data/test_task_data/loc_build.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(funtine_data_train_other_loc_build, json_file, ensure_ascii=False, indent=4)\n",
    "with open('muti_task_data/test_task_data/mul.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(funtine_data_train_mutil, json_file, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
